name: Concatenate Audio Chunks and Upload to Cloudinary

on:
  push:
    paths:
      - 'public/data/uploads/*.json'
  workflow_dispatch:
    inputs:
      upload_id:
        description: 'Upload ID to process (optional - will find incomplete uploads if not specified)'
        required: false
        type: string
      force_process:
        description: 'Force process even if upload appears completed'
        required: false
        type: boolean
        default: false
      playlist_update_only:
        description: 'Skip chunk processing and only update playlist (for completed uploads)'
        required: false
        type: boolean
        default: false

jobs:
  concat-upload:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Read upload info JSON
        id: uploadinfo
        run: |
          # Check if this is a manual trigger
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            echo "Manual workflow trigger detected"
            
            # If specific upload ID provided, use that
            if [ -n "${{ github.event.inputs.upload_id }}" ]; then
              upload_id="${{ github.event.inputs.upload_id }}"
              echo "Looking for upload ID: $upload_id"
              
              # Find the specific upload file
              for file in public/data/uploads/*.json; do
                if [ -f "$file" ]; then
                  file_upload_id=$(jq -r .uploadId "$file" 2>/dev/null || echo "null")
                  if [ "$file_upload_id" = "$upload_id" ]; then
                    json_file="$file"
                    echo "Found upload file: $json_file"
                    break
                  fi
                fi
              done
              
              if [ -z "$json_file" ]; then
                echo "Upload ID $upload_id not found. Exiting."
                exit 1
              fi
            else
              # No specific upload ID, look for incomplete uploads
              echo "Looking for incomplete uploads to process..."
              
              # Find incomplete uploads (no progress or progress != complete)
              for file in public/data/uploads/*.json; do
                if [ -f "$file" ]; then
                  progress=$(jq -r .progress "$file" 2>/dev/null || echo "null")
                  hlsUrl=$(jq -r .hlsUrl "$file" 2>/dev/null || echo "null")
                  
                  if [ "$progress" != "complete" ] || [ "$hlsUrl" = "null" ]; then
                    echo "Found incomplete upload: $file"
                    json_file="$file"
                    break
                  fi
                fi
              done
              
              if [ -z "$json_file" ]; then
                echo "No incomplete uploads found. Exiting."
                exit 0
              fi
            fi
          else
            # Automatic trigger - try to get the latest added/modified JSON file in uploads
            json_file=$(git diff --name-only ${{ github.event.before }} ${{ github.sha }} | grep 'public/data/uploads/' | tail -n1)
            
            # If no file was modified in this commit, look for incomplete uploads
            if [ -z "$json_file" ]; then
              echo "No upload JSON files were modified in this commit."
              echo "Looking for incomplete uploads to process..."
              
              # Find incomplete uploads (no progress or progress != complete)
              for file in public/data/uploads/*.json; do
                if [ -f "$file" ]; then
                  progress=$(jq -r .progress "$file" 2>/dev/null || echo "null")
                  hlsUrl=$(jq -r .hlsUrl "$file" 2>/dev/null || echo "null")
                  
                  if [ "$progress" != "complete" ] || [ "$hlsUrl" = "null" ]; then
                    echo "Found incomplete upload: $file"
                    json_file="$file"
                    break
                  fi
                fi
              done
              
              if [ -z "$json_file" ]; then
                echo "No incomplete uploads found. Exiting."
                exit 0
              fi
            fi
          fi

          if [ ! -f "$json_file" ]; then
            echo "No upload info JSON file found. Exiting."
            exit 1
          fi

          echo "JSON_FILE=$json_file" >> $GITHUB_ENV
          cat "$json_file"
          uploadId=$(jq -r .uploadId "$json_file")
          totalChunks=$(jq -r .totalChunks "$json_file")
          extension=$(jq -r .fileExtension "$json_file")

          if [ -z "$extension" ] || [ "$extension" = "null" ]; then
            echo "Extension is missing or null in $json_file. Exiting."
            exit 1
          fi

          echo "UPLOAD_ID=$uploadId" >> $GITHUB_ENV
          echo "TOTAL_CHUNKS=$totalChunks" >> $GITHUB_ENV
          echo "EXTENSION=$extension" >> $GITHUB_ENV

                    # Check if this upload is already completed
          progress=$(jq -r .progress "$json_file" 2>/dev/null || echo "null")
          existingHlsUrl=$(jq -r .hlsUrl "$json_file" 2>/dev/null || echo "null")

          # For manual triggers, check processing options
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            if [ "${{ github.event.inputs.playlist_update_only }}" = "true" ]; then
              echo "Manual trigger: Playlist update only mode"
              echo "Skipping chunk processing, will only update playlist..."
              echo "SKIP_CHUNK_PROCESSING=true" >> $GITHUB_ENV
            elif [ "${{ github.event.inputs.force_process }}" = "true" ]; then
              echo "Manual trigger with force processing enabled"
              
              # Check if upload is completed - if so, chunks are likely deleted
              if [ "$progress" = "complete" ] && [ "$existingHlsUrl" != "null" ]; then
                echo "⚠️ Warning: Upload appears completed. Chunks may have been deleted."
                echo "Force processing completed uploads may fail due to missing chunks."
                echo "Consider using playlist update only instead."
              fi
              
              echo "Processing upload regardless of completion status..."
              echo "SKIP_CHUNK_PROCESSING=false" >> $GITHUB_ENV
            else
              # Normal manual trigger - check completion status
              if [ "$progress" = "complete" ] && [ "$existingHlsUrl" != "null" ]; then
                echo "Upload already completed with HLS URL: $existingHlsUrl"
                echo "Skipping completed upload..."
                echo "SKIP_CHUNK_PROCESSING=true" >> $GITHUB_ENV
              else
                echo "Processing incomplete upload..."
                echo "SKIP_CHUNK_PROCESSING=false" >> $GITHUB_ENV
              fi
            fi
          else
            # Automatic trigger - check completion status
            if [ "$progress" = "complete" ] && [ "$existingHlsUrl" != "null" ]; then
              echo "Upload already completed with HLS URL: $existingHlsUrl"
              echo "Skipping completed upload..."
              echo "SKIP_CHUNK_PROCESSING=true" >> $GITHUB_ENV
            else
              echo "Processing incomplete upload..."
              echo "SKIP_CHUNK_PROCESSING=false" >> $GITHUB_ENV
            fi
          fi

      - name: Verify chunks exist on Cloudinary
        if: env.SKIP_CHUNK_PROCESSING != 'true'
        env:
          CLOUDINARY_CLOUD_NAME: ${{ secrets.CLOUDINARY_CLOUD_NAME }}
          CLOUDINARY_API_KEY: ${{ secrets.CLOUDINARY_API_KEY }}
          CLOUDINARY_API_SECRET: ${{ secrets.CLOUDINARY_API_SECRET }}
        run: |
          echo "Verifying chunks exist on Cloudinary before download..."
          pip install cloudinary

          for i in $(seq -f "%04g" 0 $(($TOTAL_CHUNKS - 1))); do
            chunk_public_id="podcast/chunks/$UPLOAD_ID-chunk-$i.$EXTENSION"
            echo "Checking chunk $i: $chunk_public_id"
            
            python -c "import cloudinary; import cloudinary.api; cloudinary.config(cloud_name='${{ secrets.CLOUDINARY_CLOUD_NAME }}', api_key='${{ secrets.CLOUDINARY_API_KEY }}', api_secret='${{ secrets.CLOUDINARY_API_SECRET }}'); result = cloudinary.api.resource('$chunk_public_id', resource_type='raw'); print(f'✅ Chunk $i exists: {result[\"public_id\"]} ({result[\"bytes\"]} bytes)')" || (echo "❌ Chunk $i not found" && exit 1)
          done
          echo "All chunks verified on Cloudinary!"

      - name: Download audio chunks from Cloudinary
        if: env.SKIP_CHUNK_PROCESSING != 'true'
        run: |
          echo "Starting chunk download with retry logic..."
          echo "UPLOAD_ID: $UPLOAD_ID"
          echo "TOTAL_CHUNKS: $TOTAL_CHUNKS"
          echo "EXTENSION: $EXTENSION"

          for i in $(seq -f "%04g" 0 $(($TOTAL_CHUNKS - 1))); do
            chunk_url="https://res.cloudinary.com/${{ secrets.CLOUDINARY_CLOUD_NAME }}/raw/upload/podcast/chunks/$UPLOAD_ID-chunk-$i.$EXTENSION"
            echo "Downloading chunk $i from: $chunk_url"
            
            # Retry logic for each chunk
            max_retries=5
            retry_count=0
            
            while [ $retry_count -lt $max_retries ]; do
              echo "Attempt $((retry_count + 1)) for chunk $i..."
              
              # Download with verbose output and timeout
              curl -L -v --connect-timeout 30 --max-time 60 -o chunk-$i.$EXTENSION "$chunk_url" 2>&1 | tee curl_log_$i.txt
              
              # Check if file was downloaded successfully
              if [ -s "chunk-$i.$EXTENSION" ]; then
                echo "✅ Chunk $i downloaded successfully ($(stat -c%s chunk-$i.$EXTENSION) bytes)"
                break
              else
                echo "❌ Chunk $i download failed (0 bytes)"
                rm -f chunk-$i.$EXTENSION
                retry_count=$((retry_count + 1))
                
                if [ $retry_count -lt $max_retries ]; then
                  echo "Waiting 10 seconds before retry..."
                  sleep 10
                else
                  echo "❌ Failed to download chunk $i after $max_retries attempts"
                  echo "Curl log for chunk $i:"
                  cat curl_log_$i.txt
                  exit 1
                fi
              fi
            done
          done

                    echo "All chunks downloaded successfully!"

      - name: List and verify chunk files
        if: env.SKIP_CHUNK_PROCESSING != 'true'
        run: |
          ls -lh chunk-*.$EXTENSION || echo "No chunks downloaded"
          for f in chunk-*.$EXTENSION; do
            if [ ! -s "$f" ]; then
              echo "Chunk $f is missing or empty!";
              exit 1;
            fi
          done
          echo "All chunk files are present and non-empty."

      - name: Concatenate chunks as binary
        if: env.SKIP_CHUNK_PROCESSING != 'true'
        run: |
          cat chunk-*.$EXTENSION > full_upload.$EXTENSION

      - name: Install FFmpeg
        if: env.SKIP_CHUNK_PROCESSING != 'true'
        run: sudo apt-get update && sudo apt-get install -y ffmpeg

      - name: Convert concatenated file to HLS (streamable format)
        if: env.SKIP_CHUNK_PROCESSING != 'true'
        run: |
          mkdir -p hls
          ffmpeg -y -i full_upload.$EXTENSION -codec:a aac -b:a 128k -f hls -hls_time 10 -hls_playlist_type vod -hls_segment_filename "hls/segment%03d.aac" hls/playlist.m3u8

      - name: Upload HLS files to Cloudinary
        if: env.SKIP_CHUNK_PROCESSING != 'true'
        env:
          CLOUDINARY_CLOUD_NAME: ${{ secrets.CLOUDINARY_CLOUD_NAME }}
          CLOUDINARY_API_KEY: ${{ secrets.CLOUDINARY_API_KEY }}
          CLOUDINARY_API_SECRET: ${{ secrets.CLOUDINARY_API_SECRET }}
        run: |
          pip install cloudinary
          for f in hls/*; do
            base=$(basename "$f")
            python -c "import cloudinary; import cloudinary.uploader; cloudinary.config(cloud_name='${{ secrets.CLOUDINARY_CLOUD_NAME }}', api_key='${{ secrets.CLOUDINARY_API_KEY }}', api_secret='${{ secrets.CLOUDINARY_API_SECRET }}'); result = cloudinary.uploader.upload('$f', resource_type='raw', public_id='podcast/final/${UPLOAD_ID}/$base'); print(result)"
          done

      - name: Update playlists.json with HLS playlist URL
        env:
          GITHUB_TOKEN: ${{ secrets.CH_TOKEN }}
        run: |
          echo "Starting playlist update step..."
          echo "JSON_FILE: $JSON_FILE"
          echo "UPLOAD_ID: $UPLOAD_ID"
          pip install jq
          PLAYLISTS_FILE="playlists.json"
          if [ ! -f "$PLAYLISTS_FILE" ]; then
            curl -L -o "$PLAYLISTS_FILE" "https://raw.githubusercontent.com/bl5ck/magnificent-team-podcast/main/playlists.json"
          fi
          if [ ! -f "$PLAYLISTS_FILE" ]; then
            echo "Error: $PLAYLISTS_FILE does not exist and could not be downloaded."
            exit 1
          fi

          # Always generate new HLS URL for the upload
          hlsUrl="https://res.cloudinary.com/${{ secrets.CLOUDINARY_CLOUD_NAME }}/raw/upload/podcast/final/$UPLOAD_ID/playlist.m3u8"

          # Make hlsUrl available as environment variable
          echo "HLS_URL=$hlsUrl" >> $GITHUB_ENV

          playlistId=$(jq -r .playlistId "$JSON_FILE")
          episodeId=$(jq -r .episodeId "$JSON_FILE")
          episodeTitle=$(jq -r .episodeTitle "$JSON_FILE")
          episodeDescription=$(jq -r .episodeDescription "$JSON_FILE")

          # Create new episode object
          newEpisode=$(jq -n \
            --arg id "$episodeId" \
            --arg title "$episodeTitle" \
            --arg description "$episodeDescription" \
            --arg audioUrl "$hlsUrl" \
            --arg publishDate "$(date +%Y-%m-%d)" \
            '{
              id: $id,
              title: $title,
              description: $description,
              audioUrl: $audioUrl,
              publishDate: $publishDate,
              processingStatus: "complete",
              thumbnailUrl: null,
              duration: "00:00"
            }')

          # Check if episode already exists in playlist
          existingEpisode=$(jq --arg pid "$playlistId" --arg eid "$episodeId" \
            '(.[] | select(.id == $pid).episodes[] | select(.id == $eid))' \
            "$PLAYLISTS_FILE" 2>/dev/null || echo "null")

          echo "Playlist ID: $playlistId"
          echo "Episode ID: $episodeId"
          echo "Existing episode: $existingEpisode"

          echo "Current playlist content:"
          jq '.' "$PLAYLISTS_FILE" | head -20

          if [ "$existingEpisode" = "null" ]; then
            echo "Adding new episode to playlist..."
            # Add episode to playlist
            tmpfile=$(mktemp)
            jq --arg pid "$playlistId" --argjson episode "$newEpisode" \
              '(.[] | select(.id == $pid).episodes) += [$episode]' \
              "$PLAYLISTS_FILE" > "$tmpfile" && mv "$tmpfile" "$PLAYLISTS_FILE"
            echo "Episode added successfully"
          else
            echo "Episode already exists in playlist, updating with new HLS URL..."
            # Update existing episode with new HLS URL
            tmpfile=$(mktemp)
            jq --arg pid "$playlistId" --arg eid "$episodeId" --arg audioUrl "$hlsUrl" \
              '(.[] | select(.id == $pid).episodes[] | select(.id == $eid)) |= (.audioUrl = $audioUrl | .processingStatus = "complete")' \
              "$PLAYLISTS_FILE" > "$tmpfile" && mv "$tmpfile" "$PLAYLISTS_FILE"
            echo "Episode updated successfully"
          fi

          echo "Updated playlist content:"
          jq '.' "$PLAYLISTS_FILE" | head -20

          contentBase64=$(base64 -w 0 "$PLAYLISTS_FILE")
          api_url="https://api.github.com/repos/bl5ck/magnificent-team-podcast/contents/playlists.json"
          echo "Getting file SHA from GitHub API..."
          fileSha=$(curl -s -H "Authorization: Bearer $GITHUB_TOKEN" "$api_url" | jq -r .sha)
          echo "File SHA: $fileSha"
          payload="{\"message\": \"Add new episode with HLS URL [skip ci]\", \"content\": \"$contentBase64\""
          if [ "$fileSha" != "null" ] && [ -n "$fileSha" ]; then
            payload="$payload, \"sha\": \"$fileSha\""
          fi
          payload="$payload}"
          echo "Uploading updated playlists.json to GitHub..."
          response=$(curl -X PUT -H "Authorization: Bearer $GITHUB_TOKEN" -H "Content-Type: application/json" -d "$payload" "$api_url")
          echo "GitHub API response: $response"

                    # Check if the API call was successful
          echo "Checking API response for success..."
          commit_sha=$(echo "$response" | jq -r '.commit.sha // "NOT_FOUND"')
          echo "Commit SHA: $commit_sha"

          if [ "$commit_sha" != "NOT_FOUND" ] && [ "$commit_sha" != "null" ]; then
            echo "✅ Playlist updated successfully!"
            echo "Commit SHA: $commit_sha"
            content_sha=$(echo "$response" | jq -r '.content.sha // "NOT_FOUND"')
            echo "Content SHA: $content_sha"
          else
            echo "❌ Failed to update playlist"
            echo "Error response: $response"
            exit 1
          fi

      - name: Update upload JSON with completion status
        if: env.SKIP_CHUNK_PROCESSING != 'true'
        env:
          GITHUB_TOKEN: ${{ secrets.CH_TOKEN }}
        run: |
          # Update the upload JSON with progress and HLS URL
          tmpfile=$(mktemp)
          jq --arg progress "complete" --arg hlsUrl "$HLS_URL" \
            '. + {progress: $progress, hlsUrl: $hlsUrl}' \
            "$JSON_FILE" > "$tmpfile" && mv "$tmpfile" "$JSON_FILE"

          contentBase64=$(base64 -w 0 "$JSON_FILE")
          api_url="https://api.github.com/repos/bl5ck/magnificent-team-podcast/contents/$JSON_FILE"
          fileSha=$(curl -s -H "Authorization: Bearer $GITHUB_TOKEN" "$api_url" | jq -r .sha)
          payload="{\"message\": \"Update upload JSON with completion status [skip ci]\", \"content\": \"$contentBase64\""
          if [ "$fileSha" != "null" ] && [ -n "$fileSha" ]; then
            payload="$payload, \"sha\": \"$fileSha\""
          fi
          payload="$payload}"
          curl -X PUT -H "Authorization: Bearer $GITHUB_TOKEN" -H "Content-Type: application/json" -d "$payload" "$api_url"

      - name: Delete chunk files from Cloudinary
        if: env.SKIP_CHUNK_PROCESSING != 'true'
        env:
          CLOUDINARY_CLOUD_NAME: ${{ secrets.CLOUDINARY_CLOUD_NAME }}
          CLOUDINARY_API_KEY: ${{ secrets.CLOUDINARY_API_KEY }}
          CLOUDINARY_API_SECRET: ${{ secrets.CLOUDINARY_API_SECRET }}
        run: |
          pip install cloudinary
          for i in $(seq -f "%04g" 0 $(($TOTAL_CHUNKS - 1))); do
            chunk_name="podcast/chunks/${UPLOAD_ID}-chunk-$i.${EXTENSION}"
            python -c "import cloudinary; import cloudinary.uploader; cloudinary.config(cloud_name='${{ secrets.CLOUDINARY_CLOUD_NAME }}', api_key='${{ secrets.CLOUDINARY_API_KEY }}', api_secret='${{ secrets.CLOUDINARY_API_SECRET }}'); cloudinary.uploader.destroy('$chunk_name', resource_type='raw')"
          done

      - name: Cleanup
        if: env.SKIP_CHUNK_PROCESSING != 'true'
        run: rm -f chunk-*.$EXTENSION full_upload.$EXTENSION output.mp3 output.$EXTENSION && rm -rf hls
